Framing: Key ML Terminology
https://developers.google.com/machine-learning/crash-course

--? What is supervised machine learning?
Given data input (training set), the Machine Learning System (MLS) utilizes the data input to analyze the unknown data.
In other words, having learning about many settings of characteristics of an entity, given a new incomplete setting of characteristics of the entity, what can we say about the other missing characteristics of the entity.

--? What is a label?
A label is the thing that we try to predict its value. The label could be considered as one of the characteristics that the entity has.

--? What is a feature?
A feature is one of the characteristics of the entity that we are studying. For example, for spam detector, the entity is email, and the features are words in the email text, sender's address, time of the day the email was sent.

--? What is an example?
An example is one instance of the data. We can say that it's one of the settings of the characteristics of the entity.
There are two types of example, one is labeled examples and the other is unlabeld examples. The labeled example includes all the characteristics of the entity. The unlabeled example includes all characteristics of the entity, but the one that we try to infer about. We use labeled example as a thorough understanding of the entity, to learn about the entity, to use it as a fact about the entity. Using the labeled examples, we can build a model that can infer about the missing label on the unlabeled examples. Note that example can contain false values which introduces noice and contaminates our model.

--? What is a model?
A model defines the relationship between features and label; in other words, relationship between one characteristic to other characteristics of an entity. i.e. a spam detection model might associate certain features strongly with "spam".
For each model, there are two phases of its life:
	Training means creating or learning the model. Expose the model to labeled examples so that the model can learn the relationship between one characteristic and the other characteristics of the entity.
	Inference means applying the trained model to unlabeled examples. Let the model predict the value of the characteristic when what it know only is the other characteristics of the entity. i.e. during inference, the model can predict medianHouseValue for new unlabeled examples.

--? What is the difference between regression vs. classification models?
A regression model predicts continuous values, meaning real values. i.e. What is the value of a house in California? or What is the probability that a user will click on this ad?
A classification model predicts discrete values, meaning finite set of values. i.e. Is a given email message spam or not spam? or Is this an image of a dog, a cat, or a hamster?

--? An example of linear regression.
We want to investigate the relationship between cricket's chirps per minute and the temperature in Celcius. We attempt to draw a line to capture the relationship between these two characteristics. Mathematically, we can numerically capture this relationship with this equation y = mx + b, where y is the temperature(what we want to predict), m is the slope of the line, x is the number of chirps per minute(what we use to predict), and b is the y-intercept. Converting this equation from mathematics context to machine learning context, we use a little bit different naming convention for the variables. The equation would be rewritten as y' = b + w1.x1, where y' is the predicted label, b is the bias(y-intercept), w1 is the weight of feature 1(slop of the line), and x1 is the feature.
To infer the temperature y' for a new chirps-per-minute value x1 in our example, substitute the value and calculate the equation to get the value for y'.
For a more sophisticated model, it may use multiple characteristics to predict the label. i.e The equation would be y' = b + w1.x1 + w2.x2 + w3.x3

--? What does it means when we say we are traing a model?
Training a model simply means learning (determining, figuring out) the good values for all the weights and the bias from labeled examples. In other words, we need to draw the relationship line, which requires figuring the best values for the slop and the intercept. How would we be able to determine the optimal values for the bias and the intercept? We need to use the labeled examples to estimate the values for the weights and the bias. In supervised learning, we build a model by examining many examples and attempting to find a model that minimizes loss; this is called empirical risk minimization.

--? What is loss?
Loss is the penalty for a bad prediction. Loss is the number indicating how bad the model's prediction was on a single example. If the model's prediction is prefect, the loss would be zero; otherwise, the loss is greater. The goal of training a model is to discover a set of weights and bias such that the model has low loss, on average, across all labeled examples.
Basically, loss is the difference between the line(model) and the examples.

--? How do we measure aggregate loss(differences between the model and the examples)?
We can use squared loss(a popular loss function) L2 loss. The squared loss for a single example is as follows
i.e. for a single example, we calculate the loss as follow.
squared loss L2 = the square of the difference between the label and the prediction
= (observations - prediction(x))^2 = (y- y')^2
Mean square error (MSE) is the average squared loss per example over the whole dataset. Sum up all the squared loss for all the examples, then divide by the number of examples.
MSE = SUM[all examples](y-y')^2

--? How can a model reduces the loss iteratively?
How do we modify our model to obtain a possibly better model? Given that we've created some model, tuning the model to devise a better model is always what we want to aim for. There are so many ways to tune the model, so the real trick is how we can efficiently search for the best model if possible. Key point: A machine learning model is trained by starting with guess values for the weights and bias, then iteratively modifies thoses guesses until learning the weights and bias with the lowest possible loss. Once the overall loss stops changing or at least changes are extremely slow, then we say the model has converged.

features --- model with parameters such as weights and bias -----> predict y' \
						^														v
						|Adjust parameters <------compute the loss L2 = sum all (y - y')^2
																				^
label    --------------------------------------------------------> actual  y  / 

--? How do we efficiently find the convergence point, which is the optimal value of the weight such that the loss is minimum?
Given a example (x, y) and y' = w0 + w1.x, we have L2 = (y - y') ^2 = (y - w1.x)^2 = (b - w1.a) ^2 where the value of a and b are already known, so L2 is a function of w1. Thus, we can plot the loss curve.
We can use the inefficient way, the brute force way, to calculate the loss for each of the possible values of the weight. A more efficient and popular method to find the optimal value for the weight is using gradient descent. The overall process is as follow: First, we'll pick a random value for the weight, then calculate gradient of the loss curve at that point w1 to know the direction and the magnitude of change since gradient is a vector. The gradient always points in the direction of steepest INCREASE in the loss function. The gradient descent algorithm goes the opposite direction to reduce loss as quickly as possible. Since we know the direction and the magnitude in which we can minimize the loss, we update our weight by a fraction of the gradient magnitude to further ourselves to the minimum point.

?-- What is a gradient of a function?
The gradient of a function %f is the vector of partial derivatives with respect to all of the independent variables %f
ie. if f(x, y) = e^(2y).sin(x)
then %f = (df(x, y)/dx, df(x, y)/dy) = (e^(2y).cos(x), 2e^(2y).sin(x))
note that %f points in the direction of greatest increase of the function, so -%f points in the direction of greatest decrease of the function. Where %f is a vector (a vector has the head and tail point, usually the tail is at the orgin of the coordinate system.

?-- What is learning rate?
Learning rate is a scalar that is used to control the velocity of applying gradient descent on discovering the minimum or the valley. As the gradient is a vector with magnitude and direction, instead of simply updating the weight using the magnitude, we can adjust the magnitude by multiplying it with the learning rate. If the learning rate is less than 1, we reduce the magnitude of the original gradient, and if the learning rate is greater than 1, we increase the magnitude of the original gradient. i.e if the gradient magnitude is 2.5 and the learning rate is 0.01, then the gradient descent algorithm will pick the next point 0.025 away from the previous point.

?-- What are hyperparameters?
Hyperparameters are knobs that programmers tweak in machine learning algorithms. In our example, the knob is the learning rate. If we tune the learning rate to a small number, then it may take forever to reach the minimum valley. If we tune the learning rate to a large number, it may bounce back and forth before it finally reaches the valley. The ideal learning rate in one-dimension is 1/f(x)^n (the inverse of the second derivative of f(x) at x)

?-- What are precaution when trying to set the learning rate?
Setting the learning rate too large would make the gradient descent never reaches the minimum, so the key point is finding a learning rate that is large enough such that the gradient descent converges efficiently.

?-- What is a batch in gradient descent?
A batch is the total number of examples we use to calculate the gradient in a single iteration. ie, we take 1 example, and create the loss function. With the values of x and y known, the loss function becomes the function of the weight w1, from which we can draw a graph of w1 and loss. Pick a random value for w1, we calculate the gradient for this example. Then we take another example, do the same thing, draw the graph of weight and loss, and use the same value for weight from the first example to calculate the gradient. Then we take the average of these gradients.

?-- What is stochastic gradient descent(SGD)?
Stochastic gradient descent is an extreme take on gradient descent, in which it uses only a single example(a batch size of 1) per iteration. SGD works but very noisy. Specifically, the one example in the batch is chosen at random.

?-- What is mini-batch stochastic gradient descent(mini-batch SGD)?
It is the intermediate between full-batch iteration and SGD. A mini-batch is typically between 10 and 1,000 examples, chosen at random. Mini-batch SGD reduces the amount of noise in SGD but is still more efficient than full-batch.

?-- What are the risks of overfiting?
Sometimes the model overfits the training data. An overfit model gets a low loss during training but does a poor job predicting new data. The fundamental tension of machine learning is between fitting our data well, but also fitting the data as simply as possible.
Machine learning's goal is to predict accurately on new data drawn from a hidding true probability distribution. Not how well you train the model, but how well the model can predict the label given the new data. 
The fields statistical learning theory and computational learning theory have developed generalization bounds - a statistical description of a model's ability to generalize to new data based on factors such as the complexity of the model and the model's performance on training data. The theoretical analysis provides formal guarantees under idealized assumptions, thus being difficult to apply in practice.

?-- How would we get the previously unseen data?
We divide the data set into two subsets such as training set(a subset to train a model) and test set(a subset to test the model). After we train our model using the training set, we can test our model with the test set to see how well our model generalize. Assuming the test set is large enough(split 30% training set and 70% test set?)

?-- What are the basic assumptions that guide generalization?
We need to draw examples independently and identically(i.i.d) at random from the distribution. In other words, examples don't influence each other. An example of violation of this assumption is the model bases its choice of ads on what ads the user has previously seen.
The distribution is stationary. In other words, the distribution doesn't change within the data set. An example of violation of stationarity principle is that given the data set that contains retail sales information for a year, user's purchases change seasonally, meaning that the sales concentrate in some months such as December with Thanksgiving or Christmas, and other months the sales are less likely or even worse when compared with December.
Draw examples from partitions from the same distribution.

--? What are considerations when we divide the examples to training set and test set?
Make sure that your test set meets the following two conditions: large enough to yield statistically meaningful results, and representative of data set as a whole. In other words, test set must have the same characteristics like the training set.
Our test set serves as a proxy for new data.

--? Oh I've seen surprisingly good results on my evaluation metrics, does it mean that i'm solid and good to go?
No, that might be a sign that we are accidentally training on the test set. For example, high accuracy might indicate that test data has leaked into the training set. For example, building a model that predicts whether an email is spam using subject line, email body, sender's email address as features. We split data into 80-20, then training the model. After that we evaluate the model with the test set, and we achive 99% precision. What a great job we did! No, looking carefully at the data, we see that examples in the test set are duplicates of examples in the training set. That's why we achive so high precision on the test set. That is one of the cautions that we need to pay attention to before splitting data into training set and test set. In this case, we can try to eliminate duplicates on the data set before we split the data. Unless we eliminate duplicate examples, we are no longer accurately measuring how well our model generalizes to new data. When we test our model with the test set, we evaluate the model's generalization ability.

--? What is the problem of using only training set and test set with iteration to tune the hyperparameters?
Let's look at our machine learning workflow

training model on the Training set ----------> evaluate model on test set
the tweak model according to results on Test set to obtain a better score on the next iterative test with the same test set. Finally, pick the model that does best on Test Set.
Tweak model means adjusting anything about the model you can dream up - from changing the learning rate, to adding or removing features, to designing a completely new model from scratch. At the end of this workflow, pick the model that does best on the test set. Because the best model is tuned based on the test set, adjusting hyperparameters is influenced by the test set, making the model fit to the test data. Another better approach would be partition the examples into three subsets, which are training set, validation set, and test set. So the workflow has changed to the following: first, pick the model that does best on the validation set, then double-check that model against the test set.
The test sets and validation sets wear out with repeated use, meaning that we should not try to optimize the results on the test sets and validation sets by performing so many iterations.

--? What is a feature vector?
A feature vector is the set of floating-point values comprising the examples in your data set.

--? What is feature engineering?
Feature engineering means transforming raw data into a feature vector. We convert an example to a vector. In the vector, it could be that each element refers to a single feature, or a group of elements refers to a feature(feature cross).

--? How do we map raw data to numeric values?
Mapping numeric values: 
	Integer and floating-point data don't need a special encoding because they can be multiplied by a numeric weight.
Mapping categorical values: 
	Categorical features have a discrete set of possible values. Since models cannot multiply strings by the learned weights, we use feature engineering to convert strings to numeric values. Map feature values of interests(streets of interest in the data set) to integers, and we call it vocabulary. We can map all other values that exclude previous ones(all other streets) into a catch-all other category, known as an OOV(out-of-vocabulary) bucket. Using this approach, we can map the street names to numbers: Charleston Road to 0, North Shoreline Boulevard to 1, Shorebird Way to 2, Rengstorff Avenue to 3, and everything else (OOV) to 4. However, by incorporate these index numbers directly into our model, there are some downfalls or constraints for this approach, consider a model that predicts house prices using street_name as a feature. Our model needs the flexibility of learning different weights for each street that will be added to the price estimated using the other features, meaning that the model should learn different weights for different streets. Also, we aren't accounting for cases where street_name mahy take multiple values. For example, many houses are located at the corner of two streets, and there's no way to encode that information in the street_name value if it contains a single index. To remove both these constraints, we can instead create a binary vector for each categorical feature in our model that represents values as follows: for values that apply to the example, set corresponding vector elements to 1, and set all other elements to 0. The length of this vector is equal to the number of elements in the vocabulary. This representation is called a one-hot-encoding when a single value is 1, and a multi-hot encoding when multiple values are 1. i.e street_name: "Shorebird Way" --feature engineering--> street_name feature = [0,0,0,...,1,...,0,0]. This approach effectively creates a Boolean variable for every feature value. Here, if a house is on Shorebird Way then the binary value is 1 only for Shorebird Way. Thus, the model uses only the weight for Shorebird Way. Similary, if a house is at the corner of two streets, then two binary values are set to 1, and the model uses both their respective weights.

--? What is sparse representation?
Suppose that we have 1 million different street names in our data set that we want to include as values for street_name. Explicitly creating a binary vector of 1 million elements where only 1 or elements are true is a very inefficinet representation in terms of both storage and computation time when processing these vectors. Note that if we have 100 examples, feature engineering would transform these examples to include 100*1,000,000 = 100 million elements, which is very inefficient in term of storage. In this situation, a common approach is to use a sparse representation in which only nonzero values are stored. In sparse representations, an independent model weight is still learned for each feature value.

--? What are qualities of good characteristics of an entity?
Avoid rarely used discrete feature values: 
	good feature values should appear more than 5 or so times in a data set; doing so enables a model to learn how this feature value relates to the label. That is, having many examples with the same discrete value gives the model a chance to see the feature in different settings, and in turn, determine when it's a good predictor for the label. Conversely, if a feature's value appears only once or very rarly, the model can't make predictions based on that feature.
Prefer clear and obvious meanings: 
	Each feature should have a clear and obvious meaning to anyone on the project. Conversely, the meaning of the following feature value is pretty much indecipherable to anyone but the engineer who created it. In some cases, noisy data causes unclear values.
Don't mix "magic" values with actual data: 
	Good floating-point features don't contain peculiar out-of-range discontinuities or "magic" values. For example, when the data is not available, the value is set to -1 which is out of the considerable range of probability, being from 0 to 1. To work around magic values, convert the feature into two features: one feature holds only quality ratings, never magic values. One feature holds a boolean value indicating whether or not a quality_rating was supplied. Give this boolean feature a name like is_quality_rating_defined.
Account for upstream instability: 
	The definition of a feature shouldn't change over time. For example, using city_id = "br/sao_paulo" is useful because the name won't probably change over time. However, if we refer to external index to get the value, the value may change in the future, i.e, inferred_city_cluseter: "219", maybe today 219 means "worcester", a couple months later, 219 means "lowel".

--? How should we clean data?
As an Machine Learning engineer, you'll spend enormous amount of your time tossing out bad examples and cleaning up the salvageable ones. Even a few "bad apples" can spoil a large data set.

--? How should we scale feature values?
Scaling feature values:
	Scaling means converting floating-point feature values from their natural range (100 to 900) into a standard range (0 to 1 or -1 to +1). If a feature set consists of only a single feature, then scaling provides litle to no practical benefit. If, however, a feature set consists of multiple features, then feature scaling provides the following benefits: helps gradient descent converge more quickly; helps avoid the "NaN trap", in which one number in the model becomes a NaN; helps the model learn appropriate weights for each feature. Without feature scaling, the model will pay too much attention to the features having a wider range.
One popular scaling tactic is to calculate the Z score of each value. The Z score relates the number of standard deviations away from the mean: scaledvalue = (value - mean)/stddev
For example, given: mean = 100; standard deviation = 20; original value = 130, then scaled_value = (130-100)/20 and scaled_value = 1.5. When scaling with Z scores, most scaled values will be between -3 and +3, but a few values will be a little higher or lower than that range.

--? How should we handle extreme outliers?
Problem: sometimes extreme outliers affect our graph badly.
One way would be to take the log of every value. Another way is to clip the maximum value, meaning that every values greater than some specific value will be considered as the maximum value.

--? When should we divide data into bins?
In the example of house pricing, latitude is a floating-point value. However, it doesn't make sense to prepresent latitude as a floating-point feature in our model. That's because no linear relationship exists between latitude and housting values. To make latitude a hlepful predictor, let's divide latitudes into bins. Instead of having one floating-point feature, we now have 1 distinct boolean features(LatitudeBin1, LatitudeBin2, LatitudeBin3, ..., LatitudeBin11). Having 11 separate features is somewhat inelegant, so let's unite them into a single 11-element vector. Doing so will enable us to represetn latitude 37.4 as follows: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]. Thanks to binning, our model can now learn completely different weights for each latitude. Note: adding more bins enables the model to learn different behaviors from latitude 37.4 than latitude 37.5, but only if there are sufficient examples at each tenth of a latitude. Another approach is to bin by quantile, which ensures that the number of examples in each bucket is equal. Binning by quantile completely removes the need to worry about outliers.

--? What is scrubbing?
To scrub means to clean with hard rubbing.
In real-life, many examples in data sets are unreliable due to one or more of the following: omitted values - ie. a person forgot to enter a value for a house's age; duplicate examples - ie. a server mistakenly uploaded the same logs twice; bad labels - ie. a person mislabeled a picture of an oak tree as a maple; bad feature values - ie. someone typed in an extra digit, or thermometer was left out in the sun. Once detected, we typically "fix" bad examples by removing them from the data set. To detect omitted values or duplicated examples, we can write a simple program. Detecting bad feature values or labels can be far trickier. In addition to detecting bad individual examples, we must also detect bad data in the aggregate. Histograms are a great mechanism for visualizing your data in the aggregate. In addition, getting statistics such as maximum and minimum, mean and median, and standard deviation would help looking at the data as a whole.

--? What is a synthtic feature?
A synthetic feature is a feature created by two ore more existing features.

--? What is a correlation matrix?
A correlation matrix shows pairwise correlations, both for each feature compared to the target and for each feature compared to other features.
Pearson correlation coefficient (PCC) Pearson's r, the Pearson product-moment correlation coefficient (PPMCC) or the bivariate correlation, is a measure of the linear correlation between two variables X and Y. If the value is between +1 and -1, it's total positive linear correlation, 0 is no linear correlation, -1 is total negative linear correlation. Pearson's correlation coefficient is the covariance of the two variables divided by the product of their standard deviations.
Viewing the correlation matrix, we should select features that are strongly correlated with the target. Also, we like to have features that aren't so strongly correlated with each other, so that they add independent information(remember independent vectors in linear algebra). Using this information to try removing features, and also try developing additional synthetic features, such as ratios of two raw features.

--? What is a feature cross?
A feature cross is a synthetic {feature} formed by multiplying (crossing) two or more features. Crossing combinations of features can provide predictive abilities beyond what those features can provide individually. Given a situation where we have a data set containing two types of tree, one is sick tree, and the other is healthy tree. Looking at the locations of the trees, we see that sick trees are in Quadrant I and Quadrant III, and healthy trees are in Quadrant II and Quadrant IV. The problem is that it is impossible to draw a line that can separate the sick trees from the healthy trees. Why do we need to draw a line to separate the trees though? Ok this is a new concept. When we are able to draw a line to separate sick trees and healthy trees, we say it's a linear problem; otherwise, we would say it's a nonlinear problem. To solve nonlinear problem, we need to create a feature cross. A feature cross is a synthetic feature that {encodes nonlinearity} in the feature space by multiplying two or more input features together (the term cross comes from cross product). Let's create a feature cross named x3 by crossing x1 and x2: x3 = x1.x2. We'll treat this newly minted x3 feature cross just like any other feature. So our new equation for the line would be: y = bias + weight1.x1 + weight2.x2 + weight3.x3. A linear algorithm can learn a weight for w3 just as it would for weight1 and weight2. This means that although w3 encodes nonlinear information, we don't need to change how the linear model trains to determine the value of w3.

--? What are kinds of feature crosses?
We can create many different kinds of feature crosses. For example:
A.B: a feature cross formed by multiplying the values of two features.
A.B.C.D.E: a feature cross formed by multiplying the values of five features.
A.A: a feature cross formed by squaring a single feature.
Note: Supplementing scaled linear models with feature crosses has traditionally been a efficient way to train on massive-scale data sets.

--? How do we perform feature cross with one-hot vectors?
Given an example where we have two features: country and language. A one-hot encoding of each feature generates vectors with binary features that can be interpreted as country={USA, France}, language={English, Spanish}. Then if we create a feature cross of these one-hot encodings, we'll get binary features that can be interpreted as logical conjunctions such as country:USA AND language:Spanish. Another example would be supposing you bin latitude and longitude, producing separate one-hot five-element feature vectors. A given latitude and longitude could be represented as follows: binned_latitude=[0, 0, 0, 1, 0], and binned_longitude=[0, 1, 0, 0, 0]. Suppose we create a feature cross of these two feature vectors: binned_latitude X binned_longitude. This feature cross is a 25-element one-hot vector(24 zeros and 1 one). The single 1 in the cross identifies a particular conjunction of latitude and longitude. Our model can then learn particular associations about that conjunction.
Suppose we bin latitude and longitude much more coarsely:
binned_latitude(lat)= [
0 < lat <= 10
10 < lat <= 20
20 < lat <= 30
]
binned_longitude(lon)= [
0 < lon <= 15
15 < lon <= 30
]
Creating a feature cross of those coarse bins leads to synthetic feature having the following meanings:
binned_latitude_X_longitude(lat, lon) = [
0 < lat <= 10 and 0 < lon <= 15
0 < lat <= 10 and 15 < lon <= 30
10 < lat <= 20 and 0 < lon <= 15
10 < lat <= 20 and 15 < lon <= 30
20 < lat <= 30 and 0 < lon <= 15
20 < lat <= 30 and 15 < lon <= 30
]
Another example would be our model needs to preditct how satisfied dog owners will be with dogs based on two features: behavior type(barking, crying, snuggling, etc.) and time of day. If we build a feature cross from these features: [behavior type x time of day] then we'll end up with vastly more prdictive ability than either feature on its own. For example, if a dog cries(happily) at 5:00pm when the owner returns from work will likely be a great positive predictor of owner satisfaction. Crying (miserably) at 3:00 am when the owner was sleeping soundly will likely be a strong negative predictor of owner satisfaction. Linear learners scale well to masstive data. Using feature crosses on masstive data sets is one efficient strategy for learning highly complex models. Neural networks provide another strategy.

--? What are the cautions with overcrossing?
Sometimes the feature crosses are contributing far less to the model than the normal (uncrossed) features (how do we know? just look at the weight). Using feature crossing, if we use a model that is too complicated, or much more complicated than the actual characteristics of the data, such as one with too many crosses, we give it the opportunity to fit to the noise in the training data, often at the cost of making the model perform badly on test data.

--? What does regularization mean?
Regularization means penalizing the complexity of a model to reduce overfitting. When the model is overfitting the data in the training set, we could prevent overfitting by penalizing complex models, a principle called regularization. Instead of minimizing loss (empirical risk minimization): minimize(Loss(Data|Model)), we'll now minimize loss+complexity, which is called structural risk minimization: minimize(Loss(Data|Model) + complexity(Model)). Our training optimization algorithm is now a function of two terms: the loss term, which measures how well the model fits the data, and the regularization term, which measures model complexity.

--? Why and when do we need to perform regularization?
When we plot the generalization curves, we see that the loss for training data decreases with iteration, but the loss for validation data eventually goes up, meaning that the model has been overfitting the training set, and it doesn't generalize well so that it can make a good prediction on the validation set. Why? For each iteration, the weights are adjusted so that it minimizes the loss between the model and the training data. The adjustment is influenced by the learning rate(which affect the magnitude of the gradient).

--? Generalization curve, what is that?
Generalization curve shows the loss for both the training set and validation set against the number of training iterations.

--? What is model complexity?
Model complexity as a function of the weights of all the features in the model. In this case, a feature weight with a high absolute value is more complex than a feature weight with a low absolute value. We can quantify complexity using the L2 regularization formula, which defines the regularization term as the sum of the squares of all the feature weights: L2 regularization term = ||w||^2= w1^2 + w2^2 + ... + wn^2. In this formula, weights close to zero have little effect on model complexity, while outlier weights can have a huge impact. ie. A linear model with the following weights {w1=0.2, w2=0.5, w3=5, w4=1, w5=0.25, w6=0.75} has an L2 regularization term of 26.915. Note that w3, with a squared value of 25, contributes nearly all the complexity. The sume of the squares of all five other weights adds just 1.915 to the L2 regularization term.
Model complexity as a function of the total number of features with nonzero weights.

--? How do we tune the overall impact of the regularization term?
We could do it by multiplying its value by a scale known as lambda(also called the regularization rate). That is, model developers aim to do the following: minimize(Loss(Data|Model) + lambda.complexity(Model)). Performing L2 regularization has the following effect on a model: encourages weight values toward 0(but not exactly 0), and encourages the mean of the weights toward 0, with a normal (bell-shaped or Gaussian) distribution. Increasing the lambda value strengthens the regularization effect.  Note: lambda*complexity(model) = lambda*(w1^2 + w2^2 + w3^2 + ... + wn^2)= lambda*w1^2 + lambda*w2^2 + lambda*w3^2 + ... + lambda*wn^2

--? What are some cautions when choosing a lambda value?
The goal is to strike the right balance between simplicity and training-data fit: if your lambda value is too high, your model will be simple, but you run the risk of underfitting your data. Your model won't learn enough about the training data to make useful predictions; if your lambda value is too low, your model will be more complex, and you run the risk of overfitting your data, and your model will learn too much about the particularities of the training data, and won't be able to generalize to new data. Setting lambda to zero removes regularization completely. In this case, training focuses exclusively on minimizing loss, which poses the highest possible overfitting risk. The ideal value of lambda produces a model that generalizes well to new, previously unseen data.
By learning, we mean adjusting the weights so that it generalizes well on test data. Pick one example with the values for the features and the label, try to update the weights with iterations so that the model has good combination of weights to predict well on test data.

--?
The weights of the features and some of the feature crosses have lower absolute values, which implies that model complexity drops.

--? Why logistic regression?
Problem it solves? Logistic regression is like a specialized version of linear regression, in which the output is constrainted to the range from 0 to 1.
Does it relate to anything that I know before somehow? It relates to linear regression, which is a type of regression model, which is a type of model that outputs continous values instead of classification models outputing discrete values, that outputs a continous value from a linear combination of input features.

--? What is logistic regression?
Instead of predicting exactly 0 or 1, logistic regression generates a probability - a value between 0 and 1, exclusive. For example, consider a logistic regression model for spam detection. If the model infers a value of 0.932 on a particular email emssage, it implies a 93.2% probability that the email message is spam. More precisely, it means that in the limit of infinite training examples, the set of examples for which the model predicts 0.932 will actually be spam 93.2% of the time and the remaining 6.8% will not.
Many problems require a probability estimate as output. Logistic regression is an extremely efficient mechanism for calculating probabilities. We can use the returned value from logistic regression in either of the following two ways: as is, or converted to a binary category.

--? How do we use the returned value from logistic regression "as is"?
Suppose we create a logistic regression model to predict the probability that a dog will bark during the middle of the night. We'll call that probability: p(bark | night). If the logistic regression model predicts a p(bark | night) of 0.05, then over a year, the dog's owners should be startled awake approximately 18 times: startled = p(bark | night) * nights = <-> 18 ~= 0.05 * 365.

--? How do we use the returned value from logistic regression as a binary category?
In many cases, we'll map the logistic regression output into the solution to a binary classification problem, in which the goal is to correctly predict one of two possible labels (spam or not spam)

--? How are we sure that the logistic regression model can ensure output that always falls between 0 and 1?
As it happens, a sigmoid function, defined as follows, produces output having those same characteristics: y=1/(1+e^-z)
If z represents the output of the linear (regression) layer of a model trained with logistic regression, then signmoid(z) will yield a value (a probability) between 0 and 1. In mathematical terms:
y' = 1/(1+e^(-z)), where y' is the output of the logistic regression model for a particular example, z is b + w1x1 + w2x2 + ... + wnxn, where w values are the model's learned weights and bias, and x values are the feature values for a particular example. Note taht z is also referred to as the log-odds z = log(y/(1-y))

--? Loss Function for Logistic Regression
The loss function for linear regression is squared loss. The loss function for logistic regression is Log Loss, which is defined as follows:
Log Loss = Sum all (x, y) in Dataset(-ylog(y') - (1 - y)log(1 - y')) where (x, y) in Dataset, y is the label in a labeled example every value of y must be either be 0 or 1, and y' is the predicted value, give the set of features in x.

--? What does early stopping mean?
Early stopping is a method for regularization that involves ending model training before training loss finishes decreasing. In early stopping, we end model training when the loss on a validation data set starts to increase, that is, when generalization performance worsens.

--? Classification
Logistic regression can be used for classification tasks. Logistic regression returns a probability, and we can use the returned probability "as is" or convert the returned probability to a binary value. Given an example where we have a logistic regression model returning 0.9995 for a particular email message, we can predict that the email is very likely to be spam. Conversely, another email message with a prediction score of 0.0003 on that same logistic regression model is very likely not spam. So what about an email message with a prediction score of 0.6? In order to map a logistic regression value to a binary category, we must define a classification threshold (also called the decision threshold). A value above that threshold indicates "spam"; a value below indicates "not spam". It is tempting to assume that the classification threshold should always be 0.5, but thresholds are problem-dependent, and are therefore values that you must tune (just like the learning rate -- wrong: tuning a threshold for logistic regression is different from tuning hyperparameters such as learning rate; part of choosing a threshold is assessing how much we'll suffer for making a mistake; ie mistakenly labeling a non-spam message as spam is very bad. However, mistakenly labeling a spam message as non-spam is unpleasant, but we know it's kind of acceptable).

 --? Classification: What's up with True vs. False and Positive vs. Negative
 Metrics that we'll use to evaluate classification models.
 True means that the thing indicated Positive is actually what it is.
 False means that the thing indicated Positive is not what it claims to be.
 The value True or False is based on the Reality of the event.
 Positive - a statement that tells something.
 Negative - the negation of the Positive. We can also think of it as another statement in case there are more than two classes.
A true positive is an outcome where the model correctly predicts the positive class. Similary, a true negative is an outcome where the model correctly predicts the negative class.
A false positive is an outcome where the model incorrectly predicts the positive class. Similary, a false negative is an outcome where the model incorrectly predicts the negative class.

--? Ok, let's look at one of the metrics such as accuracy
Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right.
Accuracy = (Number of correct predictions / Total number of predictions)
For binary classification, accuracy can also be calculated in terms of positives and negatives as follows:
Accuracy = (number of true positive + number of true negative) / (number of true positive + number of false positive + number of true negative + number of false negative)
let's have an example: the following model that classified 100 tumors as maligant(the positive class) or benign (the negative class):
True Positive:
	Reality - malignant; Predicted as - Malignant; Number of True Positive - 1
False Negative:
	Reality - malignant; Predicted as - benign; Number of False Negative - 8
False Positive:
	Reality - benign; predicted as - malignant; number of false positive - 1
True Negative:
	Reality - benign; predicted as - benign; number of true negative - 90
The accuracy would be calculated as (1 + 90) / (1 + 8 + 1 + 90) = 91/100 = 0.91
Inference about the result: accuracy comes out to 0.91 or 91% (91 correct predictions out of 100 total examples). That means our tumor classifier is doing a great job of identifying malignancies, isn't it?
Let's do a closer analysis of positives and negatives to gain more insight into our model's performance.
Of the 100 tumor examples, 91 are benign (90 TNs and 1 FP) and 9 are malignant (1 TP and 9 FNs). However, of the 9 malignant tumors, the model only correctly identifies 1 as malignant - a terrible outcome, as 8 out of 9 malignancies go undiagnosed!
Accuracy alone doesn't tell the full story when we're working with a class-imbalanced data set, like this one, where there is a significant disparity between the number of positive and negative labels.

--? Ok, given the accuracy metric alone is not good enough, let's look at two better metrics: precision and recall.
Precision attempts to answer the following question: What proportion of positive identifications was actually correct?
	Precision = number of True Positive / (number of True Positive + number of False Positive)
	A model that produces no false positives has a precision of 1.0.
	let's calculate the precision for the tumor example:
	Precision (for Possitive Class) = 1 / (1 + 1) = 0.5 => our model has a precision of 0.5 - in other words, when it predicts a tumor is malignant, it is correct 50% of the time.
	Precision (for Negative Class) = 90 / (8 + 90) = 90/98 ~= .92 => our model has a precision of .92, meaning when it predicts a tumor is benign, it is correct 92% of the time.
Recall attempts to answer the following question: What proportion of actual positives was identified correctly?
	Recall = true class / reality class = true possitive / (true positive + false negative)
	A model that produces no false negatives has a recall of 1.0
	let's calculate the recall for the tumor example:
	Recall (for Positive Class) = 1 / (1 + 8) = 1 / 9 = 0.11 => our model has a recall of 0.11, meaning that it correctly identifies 11% of all malignant tumors.
	Recall (for Negative Class) = 90 / (90 + 1) = 90/91 ~= 0.99 => our model has a recall of 0.99, meaning that it correctly identifies 99% of all benign tumors. 
Precision looks at the prediction, and Recall looks at the reality.

--? How do we evaluate the effectiveness of a model?
We must examine both precision and recall. One interesting relationship between precision and recall is that they are two extrems of a spectrum, meaning that improving precision typically reduces recall and vice versa. We alreadly have the logistic function to compute the probability value for each of the example on the scale from 0 to 1. Plotting on the example on the 0 to 1 scale, coloring them by their actual labels, draw the classification threshold to separate negative class and positive class.

--? What is ROC curve?
An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: True Positive Rate and False Positive Rate. True Positive Rate is a synonym for recall and is defined as follows: TPR = TP / (TP + FN). Note that this is for binary classes, sometimes we may have more than 2 classes, we can group the other classes into one group, so turning m-class problem into binary class problem. False Positive Rate is defined as follow: FPR = FP/(FP + TN)
Look at https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc for a plot of TPR vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives. To compute the points (to draw the curve) in an ROC curve, we could evaluate a logistic regression model (to assign values to examples) many times with different classification thresholds (to classify the examples), but this would be inefficient. Fortunately, there's an efficient, sorting-based algorithm that can provide this information for us, called AUC (Area Under the ROC Curve)

--? Why do calculating area under the ROC Curve help us draw the ROC curve before we even know how to plot it?
AUC measures the entire two-dimensional area underneath the entire ROC curve(think integral calculus). So how calculating the area help discover the points? AUC provides an aggregate measure of performance across all possible classification thresholds (but isn't that we need the function to draw the curve in order to calculate the area under the curve?) One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. AUC represents the probability that a random positive (green) example is positioned to the right of a random negative (red) example. Is it that we pairwise count the number of possive that are to the right of the negative? AUC ranges in value from 0 to 1 (0 means none of the positive is to the right of the negative, and 1 means all of the positive is to the right of the negative). A model whose predictions are 100% wrong has an AUC of (0.0 why? because to the right of the classification threshold are the positive, but all of it was the negative); one whose predictions are 100% correct has an AUC of 1.0 (why? because all the positive examples are to the right of all the negative examples, and we can easily select the classification threshold). Do we really know how to use AUC? Why is AUC desirable? It is desirable for the two following reasons: AUC is scale-invariant, for it measures how well predicitons are ranked, rather than their absolute values (why? because it just consider the relative position between examples on the scale, regardless whatever the value of the example has), and AUC is classification-threshold-invariant, for it measures the quality of the model's predictions irrespective of what classification threshold is chosen (regardless of the position of the classification threshold, the number counting the relative positions of the positive and the negative examples are constant). However, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases: Scale invariance is not always desirable, and classification-threshold invariance is not always desirable. 
Rank(ordinality): The ordinal position of a class in an ML problem that categorizes classes from highest to lowest. For example, a behavior ranking system could rank a dog's reqards from highest (a steak) to lowest (wilted kale).

--? What is prediction bias?
Logistic regression predictions should be unbiased, meaning "average of predictions" should ~= "average of observations"??? what is average of observations?
Prediction bias = average of predictions - average of labels in data set. Note: Prediction bias is a different quantity than bias (the b in wx + b).
The prediction bias is a quantity that measures how far apart those two averages are. A significant nonzero prediction bias tells us there is a bug somewhere in our model, as it indicates that the model is wrong about how frequently positive labels occur {the frequency of the positive labels in prediction should be close to the frequency of the positive labels in observation}. For example, let's say we know on average , 1% of all emails are spam. If we don't know anything at all about a given email, we should predict taht it's 1% likely to be spam. Similarly, a good spam model should predict on average that emails are 1% likely to be spam. If instead, the model's average predicition is 20% likelihood of being spam, we can conclude that it exhibits prediction bias. What are the possible causes of prediction bias from the model? The causes are incomplete feature set, noisy data set, buggy pipeline, biased training sample, overly strong regularization. With the bias model, we can fix it by adding calibration layer that adjusts our model's output to reduce the prediction bias. For example, if our model has +3% bias, we could add a calibration layer that lowers the mean prediction by 3%. However, adding a calibration layer is a bad idea for the following reasons: we're fixing the symptom rather than the cause, and we've built a more brittle system that we must keep up to date. Note: a good model will usually have near-zero bias. That said, a low prediction bias does not prove that our model is good. A really terrible model could have a zero prediction bias. For example, a model that just predicts the mean value for all examples would be a bad model, despite having zero bias.

--?
When our model is overly regularized, consider reducing the value of lambda. Generally, a good model does not give more weight to a particular feature. The weights are evenly distributed, and we can achieve this by applying regularization. The weights are updated by the gradient descent per iteration. How does regularization effect the model training each iteration?


--? What is the problem?
Sometimes we have feature vectors contains millions of elements, feature crossing this feature with itself, create a trillion elements feature vector. Most of the elements are 0. In a high-dimensional sparse vector, it would be nice to encourage weights to drop to exactly 0. A weight of exactly 0 essentially removes the corresponding feature from the model. So on which feature should we encourage the weight to be close to 0? How do we detect meaningless feature? We might be able to encode this idea into the optimization problem done at training time, by adding an appropriately chosen regularization term. L2 regularization encourages weights to be small, but doesn't force them to exactly 0.0 (why? because the limitation of the formula?) Why L1? L1 regularization serves as an approximation to L0, a regularization term penalizing the count of non-zero coefficient values in a model. L1 has the advantage of being convex and thus efficient to compute. So we can use L1 regularization to encourage many of the uniformative coefficients (the feature values of an example that we use to search for our weights) in our model to be exactly 0, thus saving RAM at inference time.

minimize(loss) ~ gradient_descent(loss) ~ derivative(loss)
--? What is the difference between L1 vs L2 regularization?
L2 and L1 penalize weights differently
L2 penalizes weight^2, and L2 penalizes |weight|
Consequently, L2 and L2 have a different derivatives:
the derivative of L2 is 2 * weight, and the derivative of L1 is k(a constant, whose value is independent of weight).
L2 removes x% of the weight every time, and L1 subtracts some constant from the weight every time.
L1 penalizes the absolute value of all the weights.

--? Introduction to Neural Networks?
What does nonlinear mean? Nonlinear means that you can't accurately predict a label with a model of the form b + w1.x1 + w2.x2, meaning we can't draw a line separates the two classes. In other words, the dicision surface is not a line.
We've added an activation function, adding layers has more impact. Stacking nonlinearities on nonlinearities lets us model very complicated relationships between the inputs and the predicted outputs. In brief, each layer is effectively learning a more complex, higher-level function over the raw inputs.
Hidden layers are linear layers, there are layers that are non-linear transformation layers
The issue with neural network is that it's challenging to understand how it trains itself.
Working with low-dimensional deep neural networks with a few neurons in each layer is easier to explore. In fact, we can creat visualizations to completely understand the behavior and training of such networks. 
Common activaion functions: the following sigmoid activation function converts the weighted sum to a value between 0 and 1. F(x) = 1/(1 + e^(-x))
The following rectified linear unit activation function (ReLU) often works a little better than a smooth function like the sigmoid, while also being significantly easier to compute. F(x) = max(0,x)
In fact, any mathematical function can serve as an activation function. Suppose that theta represents our activation function(ReLU, Sigmoid, or whatever).
Consequently, the value of a node in the network is given by the following formula: theta(w.x + b), where theta is a function.

--? What are components of a neural network?
A set of nodes, analogous to neurons, organized in layers.
A set of weights representing the connections between each neural network layer and the layer beneath it. The layer beneath may be another neural network layer, or some other kind of layer.
A set of biases, one for each node.
An activation function that transforms the output of each node in a layer. Different layers may have different activation functions.

--? What is decision boundary?
The separator between classes learned by a model in a binary clas or multi-class classification problems(to draw it, the model has to make prediction on all the possible input). For example, in the following image representing a binary classification problem, the decision boundary is the frontier between the orange clas and the blue class.

--? Training Neural Networks.
Backpropagation is the most common trainin algorithm for neural networks. The goal is to learn the weights of the network automatically from data such that the predicted output y.output is close to the target y.target for all inputs x.input. To measure how far we are from the goal, we use an error function E. A commonly used error function(loss function) is E(y.output, y.target) = 1/2(y.output-y.target)^2
Forward propagation
Input example (x.input, y.target)  and update the input layer of the network. For consistency, we consider the input to be like any other node but without an activation funcction so it's output is equal to its input, i.e. y1 = x.input.
[propagate]: to transmit smoething through a medium; to cause to spread out and affect a greater number or greater area.


Error derivative --- E(y.output, y.target) = 1/2(y.output - y.target)^2
The backpropagation algorithm decides how much to update each weight of the network AFTER COMPARING the predicted output with the actual ouput for a particular example(training on 1 example - stochatics gradient descent). For this, we need to compute how the error changes with respect to each weight dE/dWij. Because the error function is a function of n-weight, so we need to take n derivatives with respect to each weight independently.
Once we have the error derivatives, we can update the weights using a simple update rule: Wij = Wij - alpha.dE/dWij, where alpha is a positive constant, learning rate.
Additionally, we store how the error changes with: the total input of the node dE/dx and the output of the node dE/dy.
Now, back propagation: let's begin backpropagating the error derivatives.
E = 1/2(y.output - y.target)^2
pE/pY.output = y.output - y.target
Using the chain rule, we can compute dE/dx
pE/px = dy/dx.pE/py=df(x)/dx.pE/py where df(x)/dx = f(x)(1 - f(x)) when f(x) is the Sigmoid activation function.
As soon as we have the error derivative with respect to the total input of a node, we can get the error derivative with respect to the weights coming into that node.
pE/pWij = pXj/pWij . pE/pXj = Yj.pE/pXj
Animation of backpropagation https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/

--? Multi-Class Neural Networks: One vs. All
One vs. all provides a way to leverage binary classification. Given a classification problem with N possible solutions, a one-vs.-all solution consists of N separate binary classifiers-one binary classifier for each possible outcome. During training, the model runs through a sequence of binary classifiers, training each to answer a separate classification question. For example, given a picture of a dog, five different recognizers might be trained, four seeing the image as a negative example (not a dog) and one seeing the image as a positive example (a dog). That is:
1. Is this image an apple? No.
2. Is this miage a bear? No.
3. is this image candy? No.
4. Is this image a dog? Yes.
5. is this image an egg? No.
This approach is fairly reasonable when the total number of classes is small, but becomes increasingly inefficient as the number of classes rises.
We can create a significant more efficient one-vs.-all model with a deep neural network in which each output node represents a different class. 

--? Softmax?
Softmax assigns decimal probabilities to each class in a multi-class problem. Those decimal probabilities must add up to 1.0. This additional constraint helps training converge more quickly than it other wise would. With the above example, Softmax might produce the following likelihoods of an image belonging to a particular class:
Class 	Probability
apple 	0.001
bear 		0.04
candy	0.008
dog 		0.95
egg 		0.001
Softmax is implemented through a neural network layer just before the output layer. The Softmax layer must have the same number of nodes a the output layer.
Input ----> Layers -> Output --> Softmax Layer answering questions (# of nodes of Softmax = # of nodes of output).
Softmax equation.
Softmax Options: 
	Full Softmax is the Softmax we've been discussing; that is, Softmax calculates a probability for every possible class.
	Candidate sampling means that Softmax calculates a probability for all the positive labels but only for a random sample of negative labels. For example, if we are interested in determining whether an input image is a beagle or a bloodhound, we don't have to provide probabilities ofr every non-doggy example.
One Label vs. Many Labels
	Softmax assumes that each example is a member of exactly one class. Some examples can simutaneously be a member of multiple classes. For such examples, we may not use Softmax, and we must rely on multiple logistic regressions.
	For example, suppose our examples are images containing exactly one item - a piece of fruit. Softmax can determine the likelihood of that one item being a pear, an orange, an apple, and so on. If our examples are images containing all sorts of things - bowls of different kinds of fruit - then we will have to use multiple logistic regressions instead.

--? Embeddings
An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors, meaning dimensional reduction. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an embedding captures some of the semantics of the input by placing semantically similar inputs close together in the embedding space. An embedding can be learned and reused across models.
Collaborative filtering is the task of making predictions about the interests of a user based on interests of many other users.
As an example, let's look at the task of movie recommendation. Suppose we have 1 million users, and a list of the movies each user has watched. Our goal is to recommmend movies to users.
To solve this problem some method is needed to determine which movies are similar to each other. We can achieve this goal by embedding the moviews into a low-dimensional space created such that similar movies are nearby.

--? Categorical Input Data
Categorical data refers to input features that represent one or more discrete items from a finite set of choices. Categorical data is most efficiently represented via sparse tensors, which are tensors with evry few non-zero elements.
Sparse representations have a couple of problems that can make it hard for a model to learn effectively.
Size of Network: Huge input vectors mean a super-huge number of weights for a neural network. If there are M words in your vocabulary and N nodes in the first layer of the network above the input, we have MxN weights to train for that layer. A large number of weights causes further problems: Amount of data - teh more weights in our model, the more data we need to train effectively; amount of computation - the more weights, the more cmoputation required to train and use the model. It's easy to exceed the capabilities of our hardware. Also, lack of meaningful relations between vectors. The solution to these problems is to use embeddings, which translate large sparse vectors into a lower-dimensional space that {preserves semantic relationships}.
An embedding is a matrix in which each column is the vector that corresponds to an item in your vocabulary.

--? What is a bag of words?
A vector contain counts of the words in a enormous chunck of text. In a bag-of-words vector, several of the 500,000 nodes would have non-zero value. To get the dense vector for a sparse vector representing multiple vocabulary items(all the words in a sentence or paragraph), we could retrieve the embedding for each individual item and then add them together.