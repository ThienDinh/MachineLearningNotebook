Framing: Key ML Terminology
https://developers.google.com/machine-learning/crash-course

--? What is supervised machine learning?
Given data input (training set), the Machine Learning System (MLS) utilizes the data input to analyze the unknown data.
In other words, having learning about many settings of characteristics of an entity, given a new incomplete setting of characteristics of the entity, what can we say about the other missing characteristics of the entity.

--? What is a label?
A label is the thing that we try to predict its value. The label could be considered as one of the characteristics that the entity has.

--? What is a feature?
A feature is one of the characteristics of the entity that we are studying. For example, for spam detector, the entity is email, and the features are words in the email text, sender's address, time of the day the email was sent.

--? What is an example?
An example is one instance of the data. We can say that it's one of the settings of the characteristics of the entity.
There are two types of example, one is labeled examples and the other is unlabeld examples. The labeled example includes all the characteristics of the entity. The unlabeled example includes all characteristics of the entity, but the one that we try to infer about. We use labeled example as a thorough understanding of the entity, to learn about the entity, to use it as a fact about the entity. Using the labeled examples, we can build a model that can infer about the missing label on the unlabeled examples. Note that example can contain false values which introduces noice and contaminates our model.

--? What is a model?
A model defines the relationship between features and label; in other words, relationship between one characteristic to other characteristics of an entity. i.e. a spam detection model might associate certain features strongly with "spam".
For each model, there are two phases of its life:
	Training means creating or learning the model. Expose the model to labeled examples so that the model can learn the relationship between one characteristic and the other characteristics of the entity.
	Inference means applying the trained model to unlabeled examples. Let the model predict the value of the characteristic when what it know only is the other characteristics of the entity. i.e. during inference, the model can predict medianHouseValue for new unlabeled examples.

--? What is the difference between regression vs. classification models?
A regression model predicts continuous values, meaning real values. i.e. What is the value of a house in California? or What is the probability that a user will click on this ad?
A classification model predicts discrete values, meaning finite set of values. i.e. Is a given email message spam or not spam? or Is this an image of a dog, a cat, or a hamster?

--? An example of linear regression.
We want to investigate the relationship between cricket's chirps per minute and the temperature in Celcius. We attempt to draw a line to capture the relationship between these two characteristics. Mathematically, we can numerically capture this relationship with this equation y = mx + b, where y is the temperature(what we want to predict), m is the slope of the line, x is the number of chirps per minute(what we use to predict), and b is the y-intercept. Converting this equation from mathematics context to machine learning context, we use a little bit different naming convention for the variables. The equation would be rewritten as y' = b + w1.x1, where y' is the predicted label, b is the bias(y-intercept), w1 is the weight of feature 1(slop of the line), and x1 is the feature.
To infer the temperature y' for a new chirps-per-minute value x1 in our example, substitute the value and calculate the equation to get the value for y'.
For a more sophisticated model, it may use multiple characteristics to predict the label. i.e The equation would be y' = b + w1.x1 + w2.x2 + w3.x3

--? What does it means when we say we are traing a model?
Training a model simply means learning (determining, figuring out) the good values for all the weights and the bias from labeled examples. In other words, we need to draw the relationship line, which requires figuring the best values for the slop and the intercept. How would we be able to determine the optimal values for the bias and the intercept? We need to use the labeled examples to estimate the values for the weights and the bias. In supervised learning, we build a model by examining many examples and attempting to find a model that minimizes loss; this is called empirical risk minimization.

--? What is loss?
Loss is the penalty for a bad prediction. Loss is the number indicating how bad the model's prediction was on a single example. If the model's prediction is prefect, the loss would be zero; otherwise, the loss is greater. The goal of training a model is to discover a set of weights and bias such that the model has low loss, on average, across all labeled examples.
Basically, loss is the difference between the line(model) and the examples.

--? How do we measure aggregate loss(differences between the model and the examples)?
We can use squared loss(a popular loss function) L2 loss. The squared loss for a single example is as follows
i.e. for a single example, we calculate the loss as follow.
squared loss L2 = the square of the difference between the label and the prediction
= (observations - prediction(x))^2 = (y- y')^2
Mean square error (MSE) is the average squared loss per example over the whole dataset. Sum up all the squared loss for all the examples, then divide by the number of examples.
MSE = SUM[all examples](y-y')^2

--? How can a model reduces the loss iteratively?
How do we modify our model to obtain a possibly better model? Given that we've created some model, tuning the model to devise a better model is always what we want to aim for. There are so many ways to tune the model, so the real trick is how we can efficiently search for the best model if possible. Key point: A machine learning model is trained by starting with guess values for the weights and bias, then iteratively modifies thoses guesses until learning the weights and bias with the lowest possible loss. Once the overall loss stops changing or at least changes are extremely slow, then we say the model has converged.

features --- model with parameters such as weights and bias -----> predict y' \
						^														v
						|Adjust parameters <------compute the loss L2 = sum all (y - y')^2
																				^
label    --------------------------------------------------------> actual  y  / 

--? How do we efficiently find the convergence point, which is the optimal value of the weight such that the loss is minimum?
Given a example (x, y) and y' = w0 + w1.x, we have L2 = (y - y') ^2 = (y - w1.x)^2 = (b - w1.a) ^2 where the value of a and b are already known, so L2 is a function of w1. Thus, we can plot the loss curve.
We can use the inefficient way, the brute force way, to calculate the loss for each of the possible values of the weight. A more efficient and popular method to find the optimal value for the weight is using gradient descent. The overall process is as follow: First, we'll pick a random value for the weight, then calculate gradient of the loss curve at that point w1 to know the direction and the magnitude of change since gradient is a vector. The gradient always points in the direction of steepest INCREASE in the loss function. The gradient descent algorithm goes the opposite direction to reduce loss as quickly as possible. Since we know the direction and the magnitude in which we can minimize the loss, we update our weight by a fraction of the gradient magnitude to further ourselves to the minimum point.

?-- What is a gradient of a function?
The gradient of a function %f is the vector of partial derivatives with respect to all of the independent variables %f
ie. if f(x, y) = e^(2y).sin(x)
then %f = (df(x, y)/dx, df(x, y)/dy) = (e^(2y).cos(x), 2e^(2y).sin(x))
note that %f points in the direction of greatest increase of the function, so -%f points in the direction of greatest decrease of the function. Where %f is a vector (a vector has the head and tail point, usually the tail is at the orgin of the coordinate system.

?-- What is learning rate?
Learning rate is a scalar that is used to control the velocity of applying gradient descent on discovering the minimum or the valley. As the gradient is a vector with magnitude and direction, instead of simply updating the weight using the magnitude, we can adjust the magnitude by multiplying it with the learning rate. If the learning rate is less than 1, we reduce the magnitude of the original gradient, and if the learning rate is greater than 1, we increase the magnitude of the original gradient. i.e if the gradient magnitude is 2.5 and the learning rate is 0.01, then the gradient descent algorithm will pick the next point 0.025 away from the previous point.

?-- What are hyperparameters?
Hyperparameters are knobs that programmers tweak in machine learning algorithms. In our example, the knob is the learning rate. If we tune the learning rate to a small number, then it may take forever to reach the minimum valley. If we tune the learning rate to a large number, it may bounce back and forth before it finally reaches the valley. The ideal learning rate in one-dimension is 1/f(x)^n (the inverse of the second derivative of f(x) at x)

?-- What are precaution when trying to set the learning rate?
Setting the learning rate too large would make the gradient descent never reaches the minimum, so the key point is finding a learning rate that is large enough such that the gradient descent converges efficiently.

?-- What is a batch in gradient descent?
A batch is the total number of examples we use to calculate the gradient in a single iteration. ie, we take 1 example, and create the loss function. With the values of x and y known, the loss function becomes the function of the weight w1, from which we can draw a graph of w1 and loss. Pick a random value for w1, we calculate the gradient for this example. Then we take another example, do the same thing, draw the graph of weight and loss, and use the same value for weight from the first example to calculate the gradient. Then we take the average of these gradients.

?-- What is stochastic gradient descent(SGD)?
Stochastic gradient descent is an extreme take on gradient descent, in which it uses only a single example(a batch size of 1) per iteration. SGD works but very noisy. Specifically, the one example in the batch is chosen at random.

?-- What is mini-batch stochastic gradient descent(mini-batch SGD)?
It is the intermediate between full-batch iteration and SGD. A mini-batch is typically between 10 and 1,000 examples, chosen at random. Mini-batch SGD reduces the amount of noise in SGD but is still more efficient than full-batch.

?-- What are the risks of overfiting?
Sometimes the model overfits the training data. An overfit model gets a low loss during training but does a poor job predicting new data. The fundamental tension of machine learning is between fitting our data well, but also fitting the data as simply as possible.
Machine learning's goal is to predict accurately on new data drawn from a hidding true probability distribution. Not how well you train the model, but how well the model can predict the label given the new data. 
The fields statistical learning theory and computational learning theory have developed generalization bounds - a statistical description of a model's ability to generalize to new data based on factors such as the complexity of the model and the model's performance on training data. The theoretical analysis provides formal guarantees under idealized assumptions, thus being difficult to apply in practice.

?-- How would we get the previously unseen data?
We divide the data set into two subsets such as training set(a subset to train a model) and test set(a subset to test the model). After we train our model using the training set, we can test our model with the test set to see how well our model generalize. Assuming the test set is large enough(split 30% training set and 70% test set?)

?-- What are the basic assumptions that guide generalization?
We need to draw examples independently and identically(i.i.d) at random from the distribution. In other words, examples don't influence each other. An example of violation of this assumption is the model bases its choice of ads on what ads the user has previously seen.
The distribution is stationary. In other words, the distribution doesn't change within the data set. An example of violation of stationarity principle is that given the data set that contains retail sales information for a year, user's purchases change seasonally, meaning that the sales concentrate in some months such as December with Thanksgiving or Christmas, and other months the sales are less likely or even worse when compared with December.
Draw examples from partitions from the same distribution.

--? What are considerations when we divide the examples to training set and test set?
Make sure that your test set meets the following two conditions: large enough to yield statistically meaningful results, and representative of data set as a whole. In other words, test set must have the same characteristics like the training set.
Our test set serves as a proxy for new data.

--? Oh I've seen surprisingly good results on my evaluation metrics, does it mean that i'm solid and good to go?
No, that might be a sign that we are accidentally training on the test set. For example, high accuracy might indicate that test data has leaked into the training set. For example, building a model that predicts whether an email is spam using subject line, email body, sender's email address as features. We split data into 80-20, then training the model. After that we evaluate the model with the test set, and we achive 99% precision. What a great job we did! No, looking carefully at the data, we see that examples in the test set are duplicates of examples in the training set. That's why we achive so high precision on the test set. That is one of the cautions that we need to pay attention to before splitting data into training set and test set. In this case, we can try to eliminate duplicates on the data set before we split the data. Unless we eliminate duplicate examples, we are no longer accurately measuring how well our model generalizes to new data. When we test our model with the test set, we evaluate the model's generalization ability.

--? What is the problem of using only training set and test set with iteration to tune the hyperparameters?
Let's look at our machine learning workflow

training model on the Training set ----------> evaluate model on test set
the tweak model according to results on Test set to obtain a better score on the next iterative test with the same test set. Finally, pick the model that does best on Test Set.
Tweak model means adjusting anything about the model you can dream up - from changing the learning rate, to adding or removing features, to designing a completely new model from scratch. At the end of this workflow, pick the model that does best on the test set. Because the best model is tuned based on the test set, adjusting hyperparameters is influenced by the test set, making the model fit to the test data. Another better approach would be partition the examples into three subsets, which are training set, validation set, and test set. So the workflow has changed to the following: first, pick the model that does best on the validation set, then double-check that model against the test set.
The test sets and validation sets wear out with repeated use, meaning that we should not try to optimize the results on the test sets and validation sets by performing so many iterations.

--? What is a feature vector?
A feature vector is the set of floating-point values comprising the examples in your data set.

--? What is feature engineering?
Feature engineering means transforming raw data into a feature vector.

--? How do we map raw data to numeric values?
Mapping numeric values: 
	Integer and floating-point data don't need a special encoding because they can be multiplied by a numeric weight.
Mapping categorical values: 
	Categorical features have a discrete set of possible values. Since models cannot multiply strings by the learned weights, we use feature engineering to convert strings to numeric values. Map feature values of interests(streets of interest in the data set) to integers, and we call it vocabulary. We can map all other values that exclude previous ones(all other streets) into a catch-all other category, known as an OOV(out-of-vocabulary) bucket. Using this approach, we can map the street names to numbers: Charleston Road to 0, North Shoreline Boulevard to 1, Shorebird Way to 2, Rengstorff Avenue to 3, and everything else (OOV) to 4. However, by incorporate these index numbers directly into our model, there are some downfalls or constraints for this approach, consider a model that predicts house prices using street_name as a feature. Our model needs the flexibility of learning different weights for each street that will be added to the price estimated using the other features, meaning that the model should learn different weights for different streets. Also, we aren't accounting for cases where street_name mahy take multiple values. For example, many houses are located at the corner of two streets, and there's no way to encode that information in the street_name value if it contains a single index. To remove both these constraints, we can instead create a binary vector for each categorical feature in our model that represents values as follows: for values that apply to the example, set corresponding vector elements to 1, and set all other elements to 0. The length of this vector is equal to the number of elements in the vocabulary. This representation is called a one-hot-encoding when a single value is 1, and a multi-hot encoding when multiple values are 1. i.e street_name: "Shorebird Way" --feature engineering--> street_name feature = [0,0,0,...,1,...,0,0]. This approach effectively creates a Boolean variable for every feature value. Here, if a house is on Shorebird Way then the binary value is 1 only for Shorebird Way. Thus, the model uses only the weight for Shorebird Way. Similary, if a house is at the corner of two streets, then two binary values are set to 1, and the model uses both their respective weights.

--? What is sparse representation?
Suppose that we have 1 million different street names in our data set that we want to include as values for street_name. Explicitly creating a binary vector of 1 million elements where only 1 or elements are true is a very inefficinet representatioin in terms of both storage and computation time when processing these vectors. Note that if we have 100 examples, feature engineering would transform these examples to include 100*1,000,000 = 100 million elements, which is very inefficient in term of storage. In this situation, a common approach is to use a sparse representation in which only nonzero values are stored. In sparese representations, an independent model weight is still learned for each feature value.

--? What are qualities of good characteristics of an entity?
Avoid rarely used discrete feature values: 
	good feature values should appear more than 5 or so times in a data set; doing so enables a model to learn how this feature value relates to the label. That is, having many examples with the same discrete value gives the model a chance to see the feature in different settings, and in turn, determine when it's a good predictor for the label. Conversely, if a feature's value appears only once or very rarly, the model can't make predictions based on that feature.
Prefer clear and obvious meanings: 
	Each feature should have a clear and obvious meaning to anyone on the project. Conversely, the meaning of the following feature value is pretty much indecipherable to anyone but the engineer who created it. In some cases, noisy data causes unclear values.
Don't mix "magic" values with actual data: 
	Good floating-point features don't contain peculiar out-of-range discontinuities or "magic" values. For example, when the data is not available, the value is set to -1 which is out of the considerable range of probability, being from 0 to 1. To work around magic values, convert the feature into two features: one feature holds only quality ratings, never magic values. One feature holds a boolean value indicating whether or not a quality_rating was supplied. Give this boolean feature a name like is_quality_rating_defined.
Account for upstream instability: 
	The definition of a feature shouldn't change over time. For example, using city_id = "br/sao_paulo" is useful because the name won't probably change over time. However, if we refer to external index to get the value, the value may change in the future, i.e, inferred_city_cluseter: "219", maybe today 219 means "worcester", a couple months later, 219 means "lowel".

--? How should we clean data?
As an Machine Learning engineer, you'll spend enormous amount of your time tossing out bad examples and cleaning up the salvageable ones. Even a few "bad apples" can spoil a large data set.

--? How should we scale feature values?
Scaling feature values:
	Scaling means converting floating-point feature values from their natural range (100 to 900) into a standard range (0 to 1 or -1 to +1). If a feature set consists of only a single feature, then scaling provides litle to no practical benefit. If, however, a feature set consists of multiple features, then feature scaling provides the following benefits: helps gradient descent converge more quickly; helps avoid the "NaN trap", in which one number in the model becomes a NaN; helps the model learn appropriate weights for each feature. Without feature scaling, the model will pay too much attention to the features having a wider range.
One popular scaling tactic is to calculate the Z score of each value. The Z score relates the number of standard deviations away from the mean: scaledvalue = (value - mean)/stddev
For example, given: mean = 100; standard deviation = 20; original value = 130, then scaled_value = (130-100)/20 and scaled_value = 1.5. When scaling with Z scores, most scaled values will be between -3 and +3, but a few values will be a little higher or lower than that range.

--? How should we handle extreme outliers?
Problem: sometimes extreme outliers affect our graph badly.
One way would be to take the log of every value. Another way is to clip the maximum value, meaning that every values greater than some specific value will be considered as the maximum value.

--? When should we divide data into bins?
In the example of house pricing, latitude is a floating-point value. However, it doesn't make sense to prepresent latitude as a floating-point feature in our model. That's because no linear relationship exists between latitude and housting values. To make latitude a hlepful predictor, let's divide latitudes into bins. Instead of having one floating-point feature, we now have 1 distinct boolean features(LatitudeBin1, LatitudeBin2, LatitudeBin3, ..., LatitudeBin11). Having 11 separate features is somewhat inelegant, so let's unite them into a single 11-element vector. Doing so will enable us to represetn latitude 37.4 as follows: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]. Thanks to binning, our model can now learn completely different weights for each latitude. Note: adding more bins enables the model to learn different behaviors from latitude 37.4 than latitude 37.5, but only if there are sufficient examples at each tenth of a latitude. Another approach is to bin by quantile, which ensures that the number of examples in each bucket is equal. Binning by quantile completely removes the need to worry about outliers.

--? What is scrubbing?
To scrub means to clean with hard rubbing.
In real-life, many examples in data sets are unreliable due to one or more of the following: omitted values - ie. a person forgot to enter a value for a house's age; duplicate examples - ie. a server mistakenly uploaded the same logs twice; bad labels - ie. a person mislabeled a picture of an oak tree as a maple; bad feature values - ie. someone typed in an extra digit, or thermometer was left out in the sun. Once detected, we typically "fix" bad examples by removing them from the data set. To detect omitted values or duplicated examples, we can write a simple program. Detecting bad feature values or labels can be far trickier. In addition to detecting bad individual examples, we must also detect bad data in the aggregate. Histograms are a great mechanism for visualizing your data in the aggregate. In addition, getting statistics such as maximum and minimum, mean and median, and standard deviation would help looking at the data as a whole.

--? What is a synthtic feature?
A synthetic feature is a feature created by two ore more existing features.

--? What is a correlation matrix?
A correlation matrix shows pairwise correlations, both for each feature compared to the target and for each feature compared to other features.
Pearson correlation coefficient (PCC) Pearson's r, the Pearson product-moment correlation coefficient (PPMCC) or the bivariate correlation, is a measure of the linear correlation between two variables X and Y. If the value is between +1 and -1, it's total positive linear correlation, 0 is no linear correlation, -1 is total negative linear correlation. Pearson's correlation coefficient is the covariance of the two variables divided by the product of their standard deviations.
Viewing the correlation matrix, we should select features that are strongly correlated with the target. Also, we like to have features that aren't so strongly correlated with each other, so that they add independent information(remember independent vectors in linear algebra). Using this information to try removing features, and also try developing additional synthetic features, such as ratios of two raw features.